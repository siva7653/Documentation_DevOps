
## AWS ELB offers three types of load balancers:

Application Load Balancer (ALB): Best for HTTP and HTTPS traffic, operates at the application layer (Layer 7).
Network Load Balancer (NLB): Best for TCP traffic, operates at the transport layer (Layer 4).
Classic Load Balancer (CLB): Operates at both the request level and connection level (Layer 4 and Layer 7), suitable for older applications.
Q: How do you set up an Application Load Balancer (ALB)?
A: To set up an ALB, go to the AWS Management Console, navigate to the EC2 dashboard, and select "Load Balancers" under "Load Balancing." Click "Create Load Balancer," choose "Application Load Balancer," and follow the prompts to configure listeners, target groups, and other settings.

 Lambda

## What is AWS Lambda and how does it work?
A: AWS Lambda is a serverless compute service that runs code in response to events and automatically manages the underlying compute resources for you. You write your code, upload it to Lambda, and configure triggers (e.g., S3 events, DynamoDB updates) to invoke the code.

## How do you manage environment variables in Lambda functions?
A: Environment variables in Lambda functions can be set during function creation or update. In the AWS Management Console, go to the Lambda function, choose "Configuration," and then "Environment variables." You can add, modify, or remove environment variables there.

## what is EBS (Elastic Block Store) where we can use it.
    EBS is a persistent block storage service that provides high-performance storage for EC2 instances. Unlike instance store volumes, which are temporary and ephemeral, EBS volumes persist beyond instance termination and can be attached to different instances.

## How do you create and attach an EBS volume to an EC2 instance?
A: To create an EBS volume, go to the EC2 dashboard, select "Volumes" under "Elastic Block Store," and click "Create Volume." Specify the size, type, and availability zone. To attach it to an instance, select the volume, click "Actions," and choose "Attach Volume," then select the instance.

 VPC (Virtual Private Cloud)
## Explain about VPC
    VPC is a logically isolated network that you define in AWS. It allows you to control your own virtual network, including IP address ranges, subnets, route tables, and network gateways.
    It's used to securely connect and manage resources within AWS resources.

    Types Default VPC and Custom VPC.
    
    Types of Subnets:
    Public Subnet: A subnet that has a route to the internet through an Internet Gateway (IGW). 
                    Resources in a public subnet can directly access the internet.
    Private Subnet: A subnet that does not have a direct route to the internet.
                    Resources in a private subnet typically use a NAT Gateway or NAT Instance to access the internet.


## Explain about S3 bucket?
    S3 means simple storage service
    storing and retrieving any amount of data from anywhere on the web. It is commonly used for a wide variety of applications, including data backup and archiving, content storage and distribution, big data analytics, and more.
    An S3 bucket is a container for storing objects. Each bucket is identified by a unique name and is associated with a specific AWS Region.
    Buckets serve as the top-level namespace for storing data in S3.
    ----------------
## How do you ensure data durability in AWS S3?
    Replication: Automatically replicates data across multiple AZs within a region.
    Versioning: Keeps multiple versions of an object, so you can recover from accidental deletions or overwrites.
    Cross-Region Replication (CRR): Optionally replicate data to a different AWS region for additional durability and disaster recovery.


## SQS (Simple Queue Service)
    SQS is highly scalable and can handle any volume of messages, providing consistent and low-latency performance.
    SQS is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. You use it to send, store, and receive messages between software components at any volume.

## What is the difference between scalability and elasticity?
    Scalability refers to the system’s ability to handle increased load by adding resources. It can be either vertical (adding more power to an existing instance) or horizontal (adding more instances).
    Elasticity refers to the system’s ability to automatically adjust its resources based on current demand. It ensures that resources are scaled up or down dynamically to match the workload, optimizing cost and performance.

## How can you achieve high availability in AWS?

    Multi-AZ Deployments: Distribute resources across multiple Availability Zones (AZs) to ensure redundancy and fault tolerance.
    Elastic Load Balancing (ELB): Use ELBs to distribute traffic across multiple instances and AZs.
    Auto Scaling: Automatically adjust the number of instances based on traffic and load.
    Amazon RDS Multi-AZ: Deploy databases in multiple AZs to ensure automatic failover and data redundancy.

## What are the best practices for designing fault-tolerant systems on AWS?
    Use Multiple AZs: Distribute your instances across multiple AZs to avoid single points of failure.
    Implement Backup and Recovery: Regularly back up your data and test your recovery procedures.
    Monitor and Automate: Use AWS CloudWatch to monitor your infrastructure and set up alarms to trigger automated responses.
    Use Managed Services: Leverage AWS managed services like RDS and S3, which come with built-in fault tolerance and high availability features.

## How does AWS Auto Scaling work, and how do you configure it?
    AWS Auto Scaling helps maintain application performance and availability by automatically adjusting the number of EC2 instances in response to changing demand. To configure it:

    Create an Auto Scaling Group (ASG): Define the minimum and maximum number of instances, and specify the launch configuration or launch template.
    Define Scaling Policies: Set up policies based on CloudWatch metrics (e.g., CPU utilization) to scale in or out.
    Use Scheduled Actions: Automatically adjust the number of instances based on predefined schedules.


## What is AWS CloudFormation, and how can it be used to deploy scalable systems?
    A CloudFormation stack is a collection of AWS resources that you can manage as a single unit. All resources in a stack are defined by the stack’s CloudFormation template. You can create, update, and delete a collection of resources by managing the stack itself rather than individual resources.
    iam using cloud formation stack to create the EKS cluster
        cloud formation stack we can use json and yaml file formats.
        
        In this stack we have mention stack name | Cluster Name | Resources like VPC, subnets, internet gate way,
            and attached internet gateway to vpn | Route Table | Route table attach with Internet Gate way|
            Cluster Security Group attach with vpn | EKS cluster details | EKS Cluster Roles |Cluster Security Group

## How do you resolve a CloudFormation stack failure?
    Review Events: Check the CloudFormation Events tab to get details about which resource failed and why.
    Inspect Logs: Look at the logs of any AWS services involved to identify errors.
    Check Resource Limits: Ensure that the stack does not exceed AWS resource limits or quotas.
    Validate Template: Use the aws cloudformation validate-template command to check for syntax errors in the template.
    Update Stack: If a resource is in an unstable state, consider updating or rolling back the stack to a stable state.
    Dependency Issues: Verify that all dependencies (e.g., IAM roles, security groups) are correctly configured and available.

<!-- ## What are some common challenges when designing scalable systems, and how do you address them?
Answer:

Bottlenecks: Identify and eliminate bottlenecks by using load balancers, scaling policies, and optimizing resource configurations.
Data Consistency: Use distributed databases with eventual consistency or partitioning strategies to handle high load.
Cost Management: Implement cost-control measures like setting up budgets and using reserved instances to manage costs effectively. -->

<!-- ##  What is AWS CloudFormation, and how does it work?

A: AWS CloudFormation is a service that allows you to define and provision AWS infrastructure using a declarative template. You write the template in JSON or YAML, which describes the resources you want to create and their configurations. CloudFormation then automates the provisioning and management of these resources, ensuring that they are created consistently and in the correct order. -->

## What are the key metrics you monitor to ensure system performance and reliability?

Key metrics to monitor include:

CPU Usage: High CPU usage can indicate performance bottlenecks or insufficient resources.
Memory Usage: Monitoring memory helps identify leaks or overuse that could degrade performance.
Disk I/O: Measures read/write operations, which can affect system performance if exceeded.
Network Latency and Throughput: Important for understanding network performance and potential bottlenecks.
Error Rates: Tracks the number and types of errors occurring in the system.
Response Times: Measures how quickly the system responds to requests, crucial for user experience.
Availability/Uptime: Ensures the system meets the required availability targets.

## How do you ensure high availability in a system?

To ensure high availability:

Design Redundancy: Implement redundant components (e.g., servers, databases) to avoid single points of failure.
Use Load Balancers: Distribute traffic across multiple servers to ensure balanced load and failover capabilities.
Implement Auto-Scaling: Automatically adjust the number of resources based on current demand.
Regular Backups: Ensure data is backed up regularly and can be restored quickly.
Disaster Recovery Plan: Have a plan in place for recovering from major outages or disasters.
Geographic Distribution: Deploy resources across multiple geographic regions or availability zones.

<!-- ## What is a VPC and why is it used?

A Virtual Private Cloud (VPC) is a logically isolated network that you define within the AWS cloud. It enables you to launch AWS resources into a virtual network that you define. A VPC provides control over IP address ranges, subnets, route tables, network gateways, and security settings, allowing you to create a secure and scalable network infrastructure. -->

<!-- ## How do you configure a VPC in AWS?

To configure a VPC in AWS, follow these steps:

Create a VPC: Define the IP address range (CIDR block) and other settings.
Create Subnets: Divide the VPC IP address range into smaller segments.
Set Up Route Tables: Define how traffic should be routed within the VPC and to/from the internet.
Configure Internet Gateway (IGW) or NAT Gateway: Attach an IGW to allow internet access or use a NAT Gateway for private subnet access.
Create and Attach Security Groups: Define inbound and outbound rules to control traffic to/from resources.
Set Up Network ACLs (Access Control Lists): Implement additional layer of security at the subnet level. -->

## What are security groups and how do they work?
    Security groups are virtual firewalls that control inbound and outbound traffic to AWS resources. They work at the instance level and are stateful, meaning if you allow an incoming request from an IP, the response is automatically allowed. Security groups are associated with EC2 instances, and you can define rules based on IP protocol, port number, and source/destination IP addresses or CIDR blocks.

## What is the difference between a security group and a network ACL?
    Security Group: Operates at the instance level and is stateful. It allows you to define inbound and outbound rules. Changes to security groups are applied immediately.
    Network ACL (NACL): Operates at the subnet level and is stateless. It allows you to define rules for inbound and outbound traffic separately. NACLs are evaluated in order and apply to all instances within the subnet.

## How do you handle network traffic for a VPC?
    Route Tables: Direct traffic between subnets and to/from the internet.
    Security Groups: Control access at the instance level.
    Network ACLs: Provide an additional layer of security at the subnet level.
    NAT Gateways/Instances: Allow private subnets to access the internet while remaining private.

<!-- ## What is a VPC Peering Connection and how is it used?

A VPC Peering Connection allows you to route traffic between two VPCs using private IP addresses. This connection can be within the same AWS region or across regions (inter-region VPC peering). It’s used to enable resources in one VPC to communicate with resources in another VPC securely.
## What is a Transit Gateway and how does it differ from VPC Peering?
Answer:
A Transit Gateway is a highly scalable and managed service that allows you to connect multiple VPCs and on-premises networks through a central hub. It simplifies network management by acting as a single point of connectivity. Unlike VPC Peering, which creates point-to-point connections, a Transit Gateway provides a hub-and-spoke model, allowing multiple VPCs and networks to connect to the same transit gateway. -->

## How do you monitor and troubleshoot network issues in AWS?
    Use Amazon CloudWatch: Monitor metrics and logs related to network traffic.
    Check VPC Flow Logs: Capture information about the IP traffic going to and from network interfaces in the VPC.
    Inspect Route Tables and Security Groups: Ensure proper configuration for routing and access control.
    Utilize AWS Trusted Advisor: Get recommendations and best practices for network configurations.
<!-- ## What is a Network Load Balancer (NLB) and when would you use it?

A Network Load Balancer (NLB) operates at the TCP level (Layer 4) and is designed to handle millions of requests per second while maintaining ultra-low latencies. It is used when you need high performance and low latency, such as for applications that require a static IP address or need to handle TCP traffic efficiently. -->

## What are IAM Policies in AWS, and how do they contribute to security?
    Identity and Access Management
    permissions for users, groups, or roles.
    These policies specify what actions are allowed or denied on specific AWS resources.
    IAM policies help ensure that only authorized users can perform certain actions, providing a way to manage access control effectively.

    Managed Policies: These are standalone policies that you can attach to multiple users, groups, or roles in your AWS account. AWS provides two types:

        AWS Managed Policies: Created and managed by AWS, these policies are designed to provide permissions for common use cases.

        Customer Managed Policies: Created and managed by you, these policies provide more granular control and can be customized to meet specific requirements.

    Inline Policies: These are policies that you create and manage directly within a single user, group, or role. 
    Inline policies maintain a strict one-to-one relationship between the entity and the policy.

<!-- ## How can you implement the principle of least privilege in AWS IAM?

To implement the principle of least privilege, follow these steps:

Start with a Restricted Baseline: Begin with the least amount of permissions and only grant additional permissions as necessary.
Use IAM Roles: Assign roles to applications and services instead of using long-term credentials.
Define Fine-Grained Policies: Create policies that are specific to the actions and resources needed for each role or user.
Regularly Review Permissions: Regularly audit and review permissions and roles to ensure they align with current requirements and remove any unnecessary permissions.
Use IAM Policy Conditions: Apply conditions to policies to restrict access based on context, such as IP addresses, times of day, or MFA authentication. -->
<!-- ## What are some best practices for managing IAM users and roles?

Best practices for managing IAM users and roles include:

Avoid Root User Usage: Use the root user only for account setup and critical operations. Create and use IAM roles and users for everyday tasks.
Use Groups for Permissions: Assign permissions to IAM groups rather than individual users for easier management.
Enforce Multi-Factor Authentication (MFA): Require MFA for users, especially those with privileged access.
Regularly Rotate Access Keys: Rotate access keys regularly and remove unused keys.
Monitor and Audit: Enable AWS CloudTrail to log API calls and review logs to detect and respond to unauthorized access. -->
<!-- ## What encryption options are available in AWS to secure data at rest and in transit?

AWS provides several encryption options to secure data:

Data at Rest:

Amazon S3: Use server-side encryption (SSE) with SSE-S3, SSE-KMS (AWS Key Management Service), or SSE-C (Customer-Provided Keys).
Amazon EBS: Use encryption options available for EBS volumes and snapshots.
Amazon RDS: Use RDS encryption for database instances and snapshots.
AWS KMS: Manage encryption keys centrally using AWS KMS.
Data in Transit:

SSL/TLS: Use SSL/TLS to encrypt data transmitted over the network.
Amazon S3: Use HTTPS to encrypt data transmitted to and from S3.
AWS CloudFront: Use HTTPS for content delivery and secure communication between CloudFront and origin servers.
## How can AWS CloudTrail be used to enhance security?
Answer:
AWS CloudTrail is a service that provides logging and monitoring of API calls made in your AWS account. It enhances security by:

Providing Audit Trails: Record all API calls for auditing and compliance purposes.
Detecting Unauthorized Access: Identify and investigate suspicious activities or unauthorized access attempts.
Analyzing Usage Patterns: Review API call patterns to detect anomalies and ensure adherence to security policies.
Integrating with SIEM Tools: Integrate CloudTrail logs with Security Information and Event Management (SIEM) tools for advanced threat detection and response. -->
6. What is AWS Config, and how does it help with security compliance?
Answer:
AWS Config is a service that provides a detailed view of the configuration of AWS resources in your account. It helps with security compliance by:

Tracking Changes: Monitor and record configuration changes to AWS resources.
Evaluating Compliance: Assess compliance against predefined rules and best practices.
Generating Reports: Create compliance reports to demonstrate adherence to regulatory requirements.
Automating Remediation: Automatically remediate configuration deviations to maintain compliance.
<!-- <!-- 
## How do you ensure secure access to AWS resources when using AWS Lambda functions?
Answer:
To ensure secure access for AWS Lambda functions:

Use IAM Roles: Assign IAM roles with least privilege permissions to Lambda functions.
Enable VPC Integration: If Lambda functions need access to resources within a VPC, configure VPC access and ensure that security groups and network ACLs are properly set up.
Secure Environment Variables: Use AWS KMS to encrypt sensitive environment variables.
Apply Resource Policies: Use Lambda resource policies to restrict access to the function by IP address or other criteria.
Monitor and Audit: Use CloudWatch Logs and CloudTrail to monitor function activity and detect any unusual behavior.
 -->

Sure, let's dive into common AWS-related interview questions and potential troubleshooting and resolution strategies: -->

<!-- ## How do you troubleshoot an EC2 instance that is not reachable?

Check Instance State: Ensure the instance is running and has passed its status checks in the AWS Management Console.
Security Groups: Verify that the security group associated with the instance has the correct inbound and outbound rules.
Network ACLs: Ensure network ACLs associated with the subnet do not block traffic.
Route Tables: Verify that the route table associated with the subnet has a route to the internet if needed (e.g., through an internet gateway for public instances).
Elastic IPs and DNS: Ensure that the instance is using the correct Elastic IP or DNS name and that it hasn’t changed.
Key Pair: Confirm that you are using the correct private key to connect via SSH or RDP.
Instance Logs: Check the instance system logs and console output for any boot issues or configuration problems.
## What are the steps to debug a Lambda function failure?

Check Logs: Use AWS CloudWatch Logs to review detailed logs from your Lambda function.
Review CloudWatch Metrics: Check CloudWatch Metrics for errors, invocations, and duration to understand the function’s behavior.
Verify Permissions: Ensure the Lambda function has the appropriate IAM role and permissions to access other AWS services or resources.
Check the Function Code: Review the function code and configuration for issues, including timeouts and memory settings.
Test Locally: Use the AWS SAM CLI or other tools to test the Lambda function locally to identify any issues before deployment.
Review Error Messages: Pay attention to error messages returned by the Lambda function to pinpoint the cause of failure.
## How do you handle IAM permissions issues?


Verify Policies: Check the IAM policies attached to the user or role to ensure they have the correct permissions.
Policy Simulator: Use the IAM Policy Simulator to test and validate policies and permissions.
Review Trust Relationships: For roles, ensure the trust relationships are correctly configured to allow the necessary services or accounts to assume the role.
Check Permissions Boundaries: Ensure that any permission boundaries are not restricting the intended permissions.
CloudTrail Logs: Use AWS CloudTrail to track API calls and identify permission issues.
## What steps would you take if an RDS instance is experiencing high CPU utilization?


Monitor Metrics: Use Amazon CloudWatch to monitor CPU utilization and other performance metrics of the RDS instance.
Check Queries: Analyze slow query logs to identify any long-running or inefficient queries that might be consuming CPU resources.
Instance Size: Evaluate whether the instance size is appropriate for your workload and consider upgrading to a larger instance if necessary.
Database Optimization: Implement database optimization techniques, such as indexing and query optimization.
Read Replicas: Consider using read replicas to distribute read traffic and reduce the load on the primary instance.
Review Application Code: Ensure that the application interacting with the database is efficient and properly utilizing database connections. -->

<!-- ## What steps would you take to troubleshoot a VPC peering connection issue?


Check Peering Connection Status: Ensure that the VPC peering connection is in the “active” state.
Update Route Tables: Verify that route tables in both VPCs are updated with the correct routes to each other.
Review Security Groups: Confirm that the security groups allow inbound and outbound traffic from the peered VPC.
Check Network ACLs: Ensure network ACLs do not block traffic between the VPCs.
DNS Resolution: If DNS resolution is required, make sure the DNS settings are properly configured.
Verify CIDR Blocks: Ensure there is no overlap in CIDR blocks between the peered VPCs. -->

## What is AWS Glue, and what are its main components?

AWS Glue is a fully managed ETL (Extract, Transform, Load) service that makes it easy to prepare and load data for analytics. Its main components are:
Glue Data Catalog: A central repository to store metadata and data schemas.
Glue Crawler: Scans data in various sources to infer schemas and populate the Data Catalog.
Glue ETL Jobs: Scripts that extract data from sources, transform it, and load it into the target data store.
Glue Dev Endpoint: Provides a development environment to create and test ETL scripts.

## What is a Glue Crawler, and how does it work?
A Glue Crawler is a component that connects to your data source, extracts metadata like table definitions, schema, and data types, and then stores this information in the Glue Data Catalog. Crawlers can automatically detect and catalog different data formats (e.g., CSV, Parquet, JSON) and update the catalog when data changes.

## How do you create a Glue ETL job, and what are the options for authoring the job?
 A Glue ETL job can be created using the AWS Glue Console, AWS SDK, or AWS CLI. There are two main options for authoring the job:
Script Authoring: Write custom Python or Scala code using Glue’s pre-built libraries.
Visual ETL: Use the Glue Studio, which provides a visual interface to build ETL workflows without writing code.

## What is the Glue Data Catalog, and why is it important?
 The Glue Data Catalog is a central metadata repository where Glue stores the schema and structure of your data. It enables easy data discovery, enables schema evolution, and serves as a unified metadata store that can be accessed by other AWS services like Athena, Redshift Spectrum, and EMR.

## What are Glue Workflows, and how do they help in orchestrating ETL processes?
 Glue Workflows allow you to create and manage complex ETL processes by defining a series of jobs, crawlers, and triggers in a specific order. It provides a way to automate and manage the execution of multiple Glue jobs, enabling end-to-end data processing pipelines.

## How does AWS Glue handle schema evolution?
 AWS Glue supports schema evolution by automatically updating the Glue Data Catalog when data structures change. If a Glue Crawler detects new fields or changes in the schema, it can add them to the existing table definition without interrupting the ETL process.

## Can you explain the concept of DynamicFrames in AWS Glue?
 DynamicFrames are an abstraction in AWS Glue that provide a flexible data structure similar to Spark's DataFrames, but with additional support for semi-structured data. DynamicFrames allow easy transformation and manipulation of complex nested data, and they can be converted to and from DataFrames for compatibility with Spark.
AWS DataBrew Interview Questions

## What is AWS DataBrew, and what are its primary use cases?
 AWS DataBrew is a visual data preparation tool that allows users to clean, normalize, and transform data without writing code. It is used primarily for data preparation tasks such as data cleaning, normalization, enrichment, and data profiling, helping to prepare data for machine learning or analytics.

## How do you create a DataBrew project, and what are the key steps involved?
 To create a DataBrew project:
Select a dataset from S3, Redshift, or other supported data sources.
Create a new project by specifying the dataset and naming the project.
Use the visual interface to explore, clean, and transform the data using built-in transformations and recipes.
Save the transformations as a recipe that can be applied to other datasets.

## What are DataBrew Recipes, and how do they work?
DataBrew Recipes are a series of steps or transformations that are applied to a dataset to clean or prepare it. Each recipe consists of one or more actions such as filtering, splitting, joining, or transforming data. Recipes can be reused across multiple datasets, enabling consistent data preparation.

## Can you explain how DataBrew handles data quality and profiling?
 DataBrew provides built-in data profiling features that automatically assess the quality of data by identifying issues such as missing values, duplicates, and outliers. It generates a detailed data profile report that provides insights into the data's structure, distribution, and potential quality issues.

## What are the advantages of using AWS DataBrew over traditional ETL tools?
 AWS DataBrew offers several advantages:
No-code Interface: Allows non-technical users to prepare data without writing code.
Integration with AWS Services: Seamlessly integrates with other AWS services like S3, Redshift, and Glue.
Scalability: Can handle large datasets without the need for manual scaling.
Automated Data Quality: Provides built-in data profiling and quality checks to ensure data accuracy.
Reusable Recipes: Enables consistent data preparation across multiple datasets through reusable recipes.

## How does DataBrew integrate with other AWS services?
 AWS DataBrew integrates with services like:
S3: For data storage and access.
Redshift: For accessing and transforming data stored in Redshift clusters.
Glue Data Catalog: For using the schema information stored in the Glue Data Catalog.
AWS Lake Formation: For fine-grained access control to data stored in the data lake.

## Can you explain the concept of DataBrew Jobs and how they are used?
 DataBrew Jobs are automated processes that apply a DataBrew Recipe to a dataset to produce a transformed output. These jobs can be scheduled to run at specific intervals, allowing for automated data preparation workflows. The output can be stored in S3 or used by other services like Redshift or Glue.

## What types of data transformations can you perform with AWS DataBrew?
 AWS DataBrew offers a wide range of transformations, including:
Cleaning Operations: Removing duplicates, filling missing values, handling outliers.
String Operations: Splitting, merging, trimming, replacing.
Date and Time Operations: Parsing, formatting, and adjusting date/time values.
Aggregations: Grouping, summing, averaging.
Enrichment: Adding calculated columns, applying conditional logic.
General Questions

## How do you ensure data security and compliance when using AWS Glue and DataBrew?
 Ensuring data security and compliance involves:
IAM Policies: Applying least-privilege access using IAM roles and policies.
Data Encryption: Enabling encryption at rest using KMS keys and in-transit using SSL/TLS.
VPC Endpoints: Using VPC endpoints to securely connect to data sources.
Monitoring and Auditing: Enabling logging and monitoring with CloudTrail and CloudWatch to track data access and job execution.

## Can you describe a scenario where you would use AWS Glue and AWS DataBrew together?
 A common scenario is using DataBrew for data preparation, such as cleaning and transforming raw data, followed by AWS Glue to orchestrate complex ETL jobs that load the prepared data into a data warehouse like Redshift or a data lake in S3. DataBrew can handle the initial data cleansing, while Glue can automate the larger-scale data movement and transformation processes.